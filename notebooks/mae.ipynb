{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "\n",
    "from data import JSRTDataModule\n",
    "\n",
    "from medmnist.info import INFO\n",
    "from medmnist.dataset import MedMNIST\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patch_sequence(x):\n",
    "        patch_height, patch_width = 16, 16\n",
    "\n",
    "        # unfold 2 extracts patches from height dimension while unfold 3 extracts patches from width dimension\n",
    "        patches = x.unfold(2, patch_height, patch_height).unfold(3, patch_width, patch_width)\n",
    "        # Reshape patches to the desired format\n",
    "        patches = patches.contiguous().view(x.size(0), x.size(1), -1, patch_height, patch_width)\n",
    "        \n",
    "        # Permute to have shape (batch_size, num_patches, channels, patch_height, patch_width)\n",
    "        patches = patches.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        return patches\n",
    "\n",
    "def create_masks(patches, mask_ratio=0.75):\n",
    "        \"\"\"mask specified proportion of patches\"\"\"\n",
    "        device = patches.device\n",
    "        patches = patches.view(patches.shape[0], patches.shape[1], -1)\n",
    "        batch_size, num_patches, _, = patches.shape\n",
    "        num_masked_tokens = int(num_patches * mask_ratio)\n",
    "\n",
    "        # take num_masks random indices\n",
    "        random_values = torch.rand(batch_size, num_patches)\n",
    "\n",
    "        indices = torch.argsort(random_values, dim=1)\n",
    "        \n",
    "        mask_indices = indices[:, num_patches - num_masked_tokens:]\n",
    "\n",
    "        mask = torch.zeros(batch_size, num_patches, dtype=torch.bool, device=device)\n",
    "        mask.scatter_(1, mask_indices, True)\n",
    "\n",
    "        masked_patches = patches.clone()\n",
    "        masked_patches[mask] = 0\n",
    "        \n",
    "        ids_reverse = torch.argsort(indices, dim=1)\n",
    "\n",
    "        return masked_patches, ids_reverse, num_masked_tokens, mask\n",
    "\n",
    "def reverse_patch_sequence(x):\n",
    "\n",
    "        # fold patches back to image\n",
    "        c = 1\n",
    "        p, q = 16, 16\n",
    "        h, w = 224, 224\n",
    "        n = x.shape[0]\n",
    "        x = x.reshape(shape=(n, h // p, w // q, p, q, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(n, c, h, w))\n",
    "\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ChestMNISTDataset(MedMNIST):\n",
    "#     def __init__(self, split = 'train'):\n",
    "#         ''' Dataset class for PneumoniaMNIST.\n",
    "#         The provided init function will automatically download the necessary\n",
    "#         files at the first class initialistion.\n",
    "\n",
    "#         :param split: 'train', 'val' or 'test', select subset\n",
    "\n",
    "#         '''\n",
    "#         self.flag = \"chestmnist\"\n",
    "#         self.size = 28\n",
    "#         self.size_flag = \"\"\n",
    "#         self.root = './data/chestmnist/'\n",
    "#         self.info = INFO[self.flag]\n",
    "#         self.download()\n",
    "\n",
    "#         npz_file = np.load(os.path.join(self.root, \"chestmnist.npz\"))\n",
    "\n",
    "#         self.split = split\n",
    "\n",
    "#         # Load all the images\n",
    "#         assert self.split in ['train','val','test']\n",
    "\n",
    "#         self.imgs = npz_file[f'{self.split}_images']\n",
    "#         self.labels = npz_file[f'{self.split}_labels']\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.imgs.shape[0]\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # TASK: Fill in the blanks such that you return two tensors\n",
    "#         # of shape [1, 28, 28], img_view1 and img_view2, representing two augmented view of the images.\n",
    "\n",
    "#         image = torch.tensor(self.imgs[index]).unsqueeze(0)\n",
    "\n",
    "#         return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ChestMNISTDataModule(LightningDataModule):\n",
    "#     def __init__(self, batch_size: int = 8):\n",
    "#         super().__init__()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.train_set = ChestMNISTDataset(split='train')\n",
    "#         self.val_set = ChestMNISTDataset(split='val')\n",
    "#         self.test_set = ChestMNISTDataset(split='test')\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(dataset=self.train_set, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(dataset=self.val_set, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(dataset=self.test_set, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module = ChestMNISTDataModule(batch_size=16)\n",
    "\n",
    "# train_dataloader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = JSRTDataModule(data_dir='./data/JSRT/', batch_size=2)\n",
    "# batch = next(iter(train_dataloader))\n",
    "\n",
    "# sample = batch[5]\n",
    "# print(sample.shape)\n",
    "# patches = create_patch_sequence(sample)\n",
    "# patches = torch.randn((2, 196, 20))\n",
    "\n",
    "# masked_patches, mask_indices, num_masked_tokens, mask = create_masks(patches, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view masked image\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "\n",
    "# image_num = 0\n",
    "# image = sample[image_num].squeeze()\n",
    "\n",
    "# f, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "\n",
    "# ax[0].imshow(image, cmap=matplotlib.cm.gray)\n",
    "# ax[0].axis('off')\n",
    "# ax[0].set_title('image')\n",
    "\n",
    "# ax[1].imshow(reversed_pathced_image.squeeze(), cmap=matplotlib.cm.gray)\n",
    "# ax[1].axis('off')\n",
    "# ax[1].set_title('masked image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    if isinstance(grid_size, int):\n",
    "        grid_size = (grid_size[0], grid_size[1])\n",
    "    grid_h = np.arange(grid_size[0], dtype=np.float32)\n",
    "    grid_w = np.arange(grid_size[1], dtype=np.float32)\n",
    "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
    "    grid = np.stack(grid, axis=0)\n",
    "\n",
    "    grid = grid.reshape([2, 1, grid_size[0], grid_size[1]])\n",
    "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
    "    assert embed_dim % 2 == 0\n",
    "\n",
    "    # use half of dimensions to encode grid_h\n",
    "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
    "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
    "    return emb\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.0\n",
    "    omega = 1.0 / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum(\"m,d->md\", pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out)  # (M, D/2)\n",
    "    emb_cos = np.cos(out)  # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        embed_dim=1024,\n",
    "        num_channels=3,\n",
    "        num_heads=16,\n",
    "        depth=24,\n",
    "        decoder_embed_dim=512,\n",
    "        decoder_depth=8,\n",
    "        decoder_num_heads=16,\n",
    "        norm_layer = nn.LayerNorm,\n",
    "        mlp_ratio=4.0,\n",
    "        patch_size=16,\n",
    "        norm_pix_loss=False,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        \"\"\"Vision Transformer.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim: Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels: Number of channels of the input (3 for RGB)\n",
    "            num_heads: Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers: Number of layers to use in the Transformer\n",
    "            num_classes: Number of classes to predict\n",
    "            patch_size: Number of pixels that the patches have per dimension\n",
    "            num_patches: Maximum number of patches an image can have\n",
    "            dropout: Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = num_channels\n",
    "\n",
    "        # -------ENCODER PART------------------------------------------------------\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, num_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    embed_dim,\n",
    "                    num_heads,\n",
    "                    mlp_ratio,\n",
    "                    qkv_bias=True,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # -------DECODER PART------------------------------------------------------\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    decoder_embed_dim,\n",
    "                    decoder_num_heads,\n",
    "                    mlp_ratio,\n",
    "                    qkv_bias=True,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(decoder_depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * num_channels, bias=True)  # decoder to patch\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_2d_sincos_pos_embed(\n",
    "            embed_dim=self.pos_embed.shape[-1],\n",
    "            grid_size=self.patch_embed.grid_size,\n",
    "            cls_token=True,\n",
    "        )\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(\n",
    "            self.decoder_pos_embed.shape[-1],\n",
    "            grid_size=self.patch_embed.grid_size,\n",
    "            cls_token=True,\n",
    "        )\n",
    "        self.decoder_pos_embed.data.copy_(\n",
    "            torch.from_numpy(decoder_pos_embed).float().unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=0.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=0.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def create_patch_sequence(self, x):\n",
    "        patch_height, patch_width = self.patch_embed.patch_size[0], self.patch_embed.patch_size[1]\n",
    "\n",
    "        # unfold 2 extracts patches from height dimension while unfold 3 extracts patches from width dimension\n",
    "        patches = x.unfold(2, patch_height, patch_height).unfold(3, patch_width, patch_width)\n",
    "        # ensure patches are contiguous\n",
    "        patches = patches.contiguous().view(-1, x.size(0), x.size(1), patch_height, patch_width)\n",
    "        # permute to have shape (batch_size, num_patches, channels, patch_size, patch_size)\n",
    "        patches = patches.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        # create sequence of pixels\n",
    "        n, c, h, w = ((x.size(0), x.size(1), x.size(2) // patch_height, x.size(3) // patch_width))\n",
    "\n",
    "        x = x.reshape(shape=(n, c, h, patch_height, w, patch_width))\n",
    "        x = torch.einsum(\"nchpwq->nhwpqc\", x)\n",
    "        x = x.reshape(shape=(n, h * w, patch_height* patch_height * c))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reverse_patch_sequence(self, x):\n",
    "        # fold patches back to image\n",
    "        c = self.in_channels\n",
    "        p, q = self.patch_embed.patch_size[0], self.patch_embed.patch_size[1]\n",
    "        h, w = self.patch_embed.img_size[0], self.patch_embed.img_size[1]\n",
    "        n = x.shape[0] # batch size\n",
    "        x = x.reshape(shape=(n, h // p, w // q, p, q, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(n, c, h, w))\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def create_masks(self, patches, mask_ratio=0.75):\n",
    "        \"\"\"mask specified proportion of patches\"\"\"\n",
    "        device = patches.device\n",
    "\n",
    "        batch_size, num_patches, _, = patches.shape\n",
    "        num_masked_tokens = int(num_patches * mask_ratio)\n",
    "\n",
    "        # Generate random values and sort to get mask indices\n",
    "        random_values = torch.rand(batch_size, num_patches, device=device)\n",
    "        indices = torch.argsort(random_values, dim=1)\n",
    "\n",
    "        mask_indices = indices[:, :num_masked_tokens]\n",
    "\n",
    "        # Create mask and scatter values to mask indices\n",
    "        mask = torch.zeros(batch_size, num_patches, dtype=torch.bool, device=device)\n",
    "        mask.scatter_(1, mask_indices, True)\n",
    "\n",
    "        # Clone patches and apply mask\n",
    "        masked_patches = patches.clone()\n",
    "        masked_patches[mask] = 0\n",
    "\n",
    "        # Get reverse indices for potential reconstruction\n",
    "        ids_reverse = torch.argsort(indices, dim=1)\n",
    "\n",
    "        return masked_patches, mask, ids_reverse, num_masked_tokens\n",
    "\n",
    "    def encoder(self, x, mask_ratio=0.75):\n",
    "        # Preprocess input\n",
    "\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add positional embedding\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "\n",
    "        # perform random masking\n",
    "        masked_image, mask, mask_indices, num_masked_tokens = self.create_masks(x, mask_ratio)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(masked_image.shape[0], -1, -1)\n",
    "  \n",
    "        x = torch.cat((cls_tokens, masked_image), dim=1)\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, mask_indices, num_masked_tokens\n",
    "    \n",
    "    def decoder(self, x, mask_indices, num_masked_tokens):\n",
    "        device = x.device\n",
    "\n",
    "        x = self.decoder_embed(x)\n",
    "        \n",
    "        # # add mask tokens    \n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], num_masked_tokens, 1)\n",
    "\n",
    "        # x[:, 1:, :] removes the first token (CLS token) from x\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1).to(device)  # no cls token\n",
    "        mask_indices = mask_indices.to(device)\n",
    "        x_ = torch.gather(x_, dim=1, index=mask_indices.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "\n",
    "        target = self.create_patch_sequence(imgs)\n",
    "\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.0e-6) ** 0.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent,  mask, mask_indices, num_masked_tokens = self.encoder(imgs, mask_ratio)\n",
    "        pred = self.decoder(latent, mask_indices, num_masked_tokens)  # [N, L, p*p*1]\n",
    "        loss = self.loss(imgs, pred, mask)\n",
    "        return loss, pred, mask, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTAE(LightningModule):\n",
    "    def __init__(self, model_kwargs, learning_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # define transformer block\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, pred, mask, latent = self.model.forward(batch['image'])\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, pred, mask, latent = self.model.forward(batch['image'])\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed_everything(42, workers=True)\n",
    "\n",
    "# data = JSRTDataModule(data_dir='./data/JSRT/', batch_size=32)\n",
    "\n",
    "# model = ViTAE(\n",
    "#     model_kwargs={\n",
    "#         'img_size': 224,\n",
    "#         'embed_dim': 768,\n",
    "#         'num_channels': 1,\n",
    "#         'num_heads': 12,\n",
    "#         'depth': 14,\n",
    "#         'decoder_embed_dim': 512,\n",
    "#         'decoder_depth': 8,\n",
    "#         'decoder_num_heads': 16,\n",
    "#         'norm_layer': nn.LayerNorm,\n",
    "#         'mlp_ratio': 4.0,\n",
    "#         'patch_size': 16,\n",
    "#         'norm_pix_loss': False,\n",
    "#         'dropout': 0.0,\n",
    "#     },\n",
    "#     learning_rate=1e-3,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     max_epochs=1,\n",
    "#     accelerator='auto',\n",
    "#     devices=[0, 1],\n",
    "#     strategy='ddp_notebook',\n",
    "#     log_every_n_steps=4,\n",
    "#     check_val_every_n_epoch=50,\n",
    "#     #save_top_k=1,\n",
    "#     logger=TensorBoardLogger(save_dir='./lightning_logs/autoencoder/', name='ViTAE'),\n",
    "#     #callbacks=[ModelCheckpoint(monitor=\"val_loss\", mode='min'), TQDMProgressBar(refresh_rate=4)],\n",
    "# )\n",
    "# trainer.fit(model=model, datamodule=data)\n",
    "\n",
    "#trainer.validate(model=model, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "#trainer.test(model=model, datamodule=data, ckpt_path=trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTAE(\n",
       "  (model): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder_embed): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (decoder_blocks): ModuleList(\n",
       "      (0-9): 10 x Block(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder_pred): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:1')\n",
    "\n",
    "saved_model = ViTAE.load_from_checkpoint('/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/lightning_logs/autoencoder/ViTAE/version_54/checkpoints/epoch=799-step=4800.ckpt',\n",
    "    model_kwargs={\n",
    "        'img_size': 224,\n",
    "        'embed_dim': 1024,\n",
    "        'num_channels': 1,\n",
    "        'num_heads': 16,\n",
    "        'depth': 16,\n",
    "        'decoder_embed_dim': 512,\n",
    "        'decoder_depth': 10,\n",
    "        'decoder_num_heads': 16,\n",
    "        'norm_layer': nn.LayerNorm,\n",
    "        'mlp_ratio': 4.0,\n",
    "        'patch_size': 16,\n",
    "        'norm_pix_loss': False,\n",
    "        'dropout': 0.0,\n",
    "    },\n",
    "    learning_rate=1e-4,\n",
    "    map_location = DEVICE\n",
    "    )\n",
    "\n",
    "saved_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m saved_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mtest_dataloader()))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     encoded_embeddings \u001b[38;5;241m=\u001b[39m saved_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(DEVICE)) \u001b[38;5;66;03m# [batch_size, num_patches + 1, embed_dim]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "saved_model.eval()\n",
    "batch = next(iter(data.test_dataloader()))\n",
    "with torch.no_grad():\n",
    "    encoded_embeddings = saved_model.model.encoder(batch['image'].to(DEVICE)) # [batch_size, num_patches + 1, embed_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#from matplotlib.cm import ScalarMappable\n",
    "image_num = 0\n",
    "image = batch['image'][image_num].squeeze()\n",
    "\n",
    "f, ax = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "ax[0].imshow(image, cmap=matplotlib.cm.gray)\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('image')\n",
    "\n",
    "masked_patches, _, masked_indices, num_masked_tokens = create_masks(create_patch_sequence(batch['image']), 0.75)\n",
    "masked_patches = reverse_patch_sequence(masked_patches[0, :, :].unsqueeze(0))\n",
    "\n",
    "ax[1].imshow(masked_patches.squeeze().cpu(), cmap=matplotlib.cm.gray)\n",
    "ax[1].axis('off')\n",
    "ax[1].set_title('e.g. masked image')\n",
    "\n",
    "# reconstructed image\n",
    "with torch.no_grad():\n",
    "    latent, mask, mask_indices, num_masked_tokens = saved_model.model.encoder(batch['image'][0].unsqueeze(0).to(DEVICE))\n",
    "    decoded_image = saved_model.model.decoder(latent, mask_indices, num_masked_tokens)\n",
    "reconstructed_image = reverse_patch_sequence(decoded_image)\n",
    "ax[2].imshow(reconstructed_image.squeeze().cpu().detach().numpy(), cmap=matplotlib.cm.gray)\n",
    "ax[2].axis('off')\n",
    "ax[2].set_title('reconstructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store the attention weights\n",
    "attention_weights = []\n",
    "\n",
    "# Define a hook to capture attention weights\n",
    "def get_attention_weights(module, input, output):\n",
    "    attention = module.attn_drop.detach().cpu().numpy()\n",
    "    attention_weights.append(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 187/187 [00:00<00:00, 27002.27it/s]\n",
      "Loading Data: 100%|██████████| 10/10 [00:00<00:00, 20039.68it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 50/50 [00:00<00:00, 30539.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention(\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (q_norm): Identity()\n",
      "  (k_norm): Identity()\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ") (tensor([[[ 7.8916e-02,  9.5279e-01,  1.8710e+00,  ...,  6.7404e-01,\n",
      "           2.0214e+00,  2.1062e+00],\n",
      "         [-6.6779e-01, -5.2936e-01, -1.5531e+00,  ...,  1.4771e+00,\n",
      "           7.8221e-01,  1.4951e+00],\n",
      "         [ 7.4483e-04,  5.9393e-04, -3.6893e-04,  ..., -4.3520e-05,\n",
      "          -7.5248e-04, -3.4184e-04],\n",
      "         ...,\n",
      "         [ 7.4483e-04,  5.9393e-04, -3.6893e-04,  ..., -4.3520e-05,\n",
      "          -7.5248e-04, -3.4184e-04],\n",
      "         [ 7.4483e-04,  5.9393e-04, -3.6893e-04,  ..., -4.3520e-05,\n",
      "          -7.5248e-04, -3.4184e-04],\n",
      "         [ 7.4483e-04,  5.9393e-04, -3.6893e-04,  ..., -4.3520e-05,\n",
      "          -7.5248e-04, -3.4184e-04]]], device='cuda:1'),) tensor([[[ 0.2668,  0.0719,  0.0276,  ..., -0.1156,  0.1562,  0.0749],\n",
      "         [ 0.2113,  0.1012,  0.2220,  ..., -0.6692, -0.1803, -0.3428],\n",
      "         [ 0.1342,  0.1710,  0.1195,  ..., -0.1301,  0.0677,  0.0860],\n",
      "         ...,\n",
      "         [ 0.1342,  0.1710,  0.1195,  ..., -0.1301,  0.0677,  0.0860],\n",
      "         [ 0.1342,  0.1710,  0.1195,  ..., -0.1301,  0.0677,  0.0860],\n",
      "         [ 0.1342,  0.1710,  0.1195,  ..., -0.1301,  0.0677,  0.0860]]],\n",
      "       device='cuda:1')\n",
      "Attention(\n",
      "  (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "  (q_norm): Identity()\n",
      "  (k_norm): Identity()\n",
      "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "  (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      ") (tensor([[[ 0.3047,  0.8824, -0.0343,  ..., -0.1306, -1.0852,  0.2339],\n",
      "         [ 0.7693, -1.0020, -0.2526,  ...,  0.0270,  0.6352,  2.2236],\n",
      "         [-0.2173,  0.3838, -0.8586,  ...,  0.0270, -0.2054, -0.1978],\n",
      "         ...,\n",
      "         [-0.2173,  0.3838, -0.8586,  ...,  0.0270, -0.2054, -0.1978],\n",
      "         [-0.2173,  0.3838, -0.8586,  ...,  0.0270, -0.2054, -0.1978],\n",
      "         [-0.2173,  0.3838, -0.8586,  ...,  0.0270, -0.2054, -0.1978]]],\n",
      "       device='cuda:1'),) tensor([[[ 1.1384, -0.0928, -0.2983,  ...,  0.3311,  1.1504, -0.0197],\n",
      "         [ 0.3554, -0.1002, -0.5759,  ...,  0.5292,  0.6155, -0.2138],\n",
      "         [ 1.1701,  0.1502, -0.5953,  ...,  0.3314,  1.2719, -0.1932],\n",
      "         ...,\n",
      "         [ 1.1701,  0.1502, -0.5953,  ...,  0.3314,  1.2719, -0.1932],\n",
      "         [ 1.1701,  0.1502, -0.5953,  ...,  0.3314,  1.2719, -0.1932],\n",
      "         [ 1.1701,  0.1502, -0.5953,  ...,  0.3314,  1.2719, -0.1932]]],\n",
      "       device='cuda:1')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dropout' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m saved_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mregister_forward_hook(get_attention_weights)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     latent, mask, mask_indices, num_masked_tokens \u001b[38;5;241m=\u001b[39m \u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#decoded_image = saved_model.model.decoder(latent, mask_indices, num_masked_tokens)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 204\u001b[0m, in \u001b[0;36mVisionTransformer.encoder\u001b[0;34m(self, x, mask_ratio)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Apply Transforrmer\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 204\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, mask, mask_indices, num_masked_tokens\n",
      "File \u001b[0;32m/vol/bitbucket/bc1623/project/uncertainty_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/bc1623/project/uncertainty_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/bc1623/project/uncertainty_env/lib/python3.10/site-packages/timm/models/vision_transformer.py:165\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 165\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    166\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/vol/bitbucket/bc1623/project/uncertainty_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/bc1623/project/uncertainty_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1595\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1595\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m, in \u001b[0;36mget_attention_weights\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_attention_weights\u001b[39m(module, \u001b[38;5;28minput\u001b[39m, output):\n\u001b[0;32m----> 6\u001b[0m     attention \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_drop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      7\u001b[0m     attention_weights\u001b[38;5;241m.\u001b[39mappend(attention)\n",
      "File \u001b[0;32m/vol/bitbucket/bc1623/project/uncertainty_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dropout' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "data = JSRTDataModule(data_dir='./data/JSRT/', batch_size=2)\n",
    "batch = next(iter(data.test_dataloader()))\n",
    "# Register hooks to all attention layers\n",
    "#for blk in saved_model.model.blocks:\n",
    "saved_model.model.blocks[1].attn.register_forward_hook(get_attention_weights)\n",
    "\n",
    "with torch.no_grad():\n",
    "    latent, mask, mask_indices, num_masked_tokens = saved_model.model.encoder(batch['image'][0].unsqueeze(0).to(DEVICE))\n",
    "    #decoded_image = saved_model.model.decoder(latent, mask_indices, num_masked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot attention weights\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "attention = attention_weights[0][0, 0]  # First block, first head\n",
    "print(attention.shape)\n",
    "attention = attention.reshape(32, 32)\n",
    "# Plot the attention map\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(attention, cmap='viridis')\n",
    "plt.title('Attention Map of the First Head in the First Block')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=3072, bias=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model.model.blocks[0].attn.qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertainty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

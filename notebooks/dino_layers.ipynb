{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from models.encoders import DinoViT_8\n",
    "from models.data import SynthCardDataModule, JSRTDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /homes/bc1623/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 210/210 [00:00<00:00, 72535.93it/s]\n",
      "Loading Data: 100%|██████████| 12/12 [00:00<00:00, 43314.67it/s]\n",
      "Loading Data: 100%|██████████| 25/25 [00:00<00:00, 57080.89it/s]\n"
     ]
    }
   ],
   "source": [
    "dinovit8 = DinoViT_8()\n",
    "\n",
    "# data = SynthCardDataModule(batch_size=1, rate_maps=1.0, augmentation=False, cache=True)\n",
    "data = JSRTDataModule(batch_size=5, augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dinovit8(batch['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_maps(model, x, layers_to_visualize):\n",
    "    # Forward pass through the model to get attention maps\n",
    "    attention_maps = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # Assuming the module output is the attention map\n",
    "        # print(len(output)) # 2\n",
    "        attention_maps.append(output[1])\n",
    "\n",
    "    # Register hooks to capture attention maps from specified layers\n",
    "    hooks = []\n",
    "    for block in model.blocks:\n",
    "        hook = block.attn.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # Forward pass to compute attention\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "\n",
    "    # Remove hooks after extraction\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # Visualize the attention maps\n",
    "    for idx, att_map in enumerate(attention_maps):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        # plt.title(f'Layer {layers_to_visualize[idx]} Attention Map')\n",
    "        plt.imshow(att_map.squeeze().sum(dim=0)[0, 1:].reshape(14, 14).cpu().numpy(), cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /homes/bc1623/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[14, 14]' is invalid for input of size 784",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebookresearch/dino:main\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdino_vits8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mvisualize_attention_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m, in \u001b[0;36mvisualize_attention_maps\u001b[0;34m(model, x, layers_to_visualize)\u001b[0m\n\u001b[1;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# plt.title(f'Layer {layers_to_visualize[idx]} Attention Map')\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43matt_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar()\n\u001b[1;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[14, 14]' is invalid for input of size 784"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')\n",
    "image = batch['image'][0].unsqueeze(0).repeat(1, 3, 1, 1)\n",
    "visualize_attention_maps(model, image, [0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(model.blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "# \n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "# \n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "import colorsys\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import skimage.io\n",
    "from skimage.measure import find_contours\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms as pth_transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "import vision_transformer as vits\n",
    "\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = image[:, :, c] * (1 - alpha * mask) + alpha * mask * color[c] * 255\n",
    "    return image\n",
    "\n",
    "\n",
    "def random_colors(N, bright=True):\n",
    "    \"\"\"\n",
    "    Generate random colors.\n",
    "    \"\"\"\n",
    "    brightness = 1.0 if bright else 0.7\n",
    "    hsv = [(i / N, 1, brightness) for i in range(N)]\n",
    "    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
    "    random.shuffle(colors)\n",
    "    return colors\n",
    "\n",
    "\n",
    "def display_instances(image, mask, fname=\"test\", figsize=(5, 5), blur=False, contour=True, alpha=0.5):\n",
    "    fig = plt.figure(figsize=figsize, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    N = 1\n",
    "    mask = mask[None, :, :]\n",
    "    # Generate random colors\n",
    "    colors = random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    margin = 0\n",
    "    ax.set_ylim(height + margin, -margin)\n",
    "    ax.set_xlim(-margin, width + margin)\n",
    "    ax.axis('off')\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    for i in range(N):\n",
    "        color = colors[i]\n",
    "        _mask = mask[i]\n",
    "        # if blur:\n",
    "        #     _mask = cv2.blur(_mask,(10,10))\n",
    "        # Mask\n",
    "        masked_image = apply_mask(masked_image, _mask, color, alpha)\n",
    "        # Mask Polygon\n",
    "        # Pad to ensure proper polygons for masks that touch image edges.\n",
    "        if contour:\n",
    "            padded_mask = np.zeros((_mask.shape[0] + 2, _mask.shape[1] + 2))\n",
    "            padded_mask[1:-1, 1:-1] = _mask\n",
    "            contours = find_contours(padded_mask, 0.5)\n",
    "            for verts in contours:\n",
    "                # Subtract the padding and flip (y, x) to (x, y)\n",
    "                verts = np.fliplr(verts) - 1\n",
    "                p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "                ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8), aspect='auto')\n",
    "    fig.savefig(fname)\n",
    "    print(f\"{fname} saved.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /homes/bc1623/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 785, 785])\n",
      "12\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head0.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head1.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head2.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head3.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head4.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head5.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head6.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head7.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head8.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head9.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head10.png saved.\n",
      "/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/attn-head11.png saved.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb8', pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "img = batch['image'][1].unsqueeze(0).repeat(1, 3, 1, 1)\n",
    "transform = pth_transforms.Compose([\n",
    "    # pth_transforms.Resize(128),\n",
    "    # pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "img = transform(img)\n",
    "# make the image divisible by the patch size\n",
    "#w, h = img.shape[1] - img.shape[1] % 8, img.shape[2] - img.shape[2] % 8\n",
    "# img = img[:, :, :w, :h]#.unsqueeze(0)\n",
    "# print(w, h)\n",
    "\n",
    "w_featmap = img.shape[-2] // 8\n",
    "h_featmap = img.shape[-1] // 8\n",
    "\n",
    "with torch.no_grad():\n",
    "    attentions = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "nh = attentions.shape[1] # number of head\n",
    "print(attentions.shape) # torch.Size([1, 6, 257, 257])\n",
    "# we keep only the output patch attention\n",
    "attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "\n",
    "# if args.threshold is not None:\n",
    "#     # we keep only a certain percentage of the mass\n",
    "#     val, idx = torch.sort(attentions)\n",
    "#     val /= torch.sum(val, dim=1, keepdim=True)\n",
    "#     cumval = torch.cumsum(val, dim=1)\n",
    "#     th_attn = cumval > (1 - None)\n",
    "#     idx2 = torch.argsort(idx)\n",
    "#     for head in range(nh):\n",
    "#         th_attn[head] = th_attn[head][idx2[head]]\n",
    "#     th_attn = th_attn.reshape(nh, w_featmap, h_featmap).float()\n",
    "#     # interpolate\n",
    "#     th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
    "attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "# save attentions heatmaps\n",
    "os.makedirs('/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/', exist_ok=True)\n",
    "torchvision.utils.save_image(torchvision.utils.make_grid(img, normalize=True, scale_each=True), os.path.join('/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/', \"img.png\"))\n",
    "print(nh)\n",
    "for j in range(nh):\n",
    "    fname = os.path.join('/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/', \"attn-head\" + str(j) + \".png\")\n",
    "    plt.imsave(fname=fname, arr=attentions[j], format='png')\n",
    "    print(f\"{fname} saved.\")\n",
    "\n",
    "# if args.threshold is not None:\n",
    "#     image = skimage.io.imread(os.path.join('/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/', \"img.png\"))\n",
    "#     for j in range(nh):\n",
    "#         display_instances(image, th_attn[j], fname=os.path.join('/vol/bitbucket/bc1623/project/semi_supervised_uncertainty/dino_attention_maps/', \"mask_th\" + str(None) + \"_head\" + str(j) +\".png\"), blur=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertainty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotAttention(torch.nn.Module):\n",
    "    def __init__(self, num_slots: int, dim: int, num_iterations: int, hidden_dim: int = 256):\n",
    "        super(SlotAttention, self).__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.num_iterations = num_iterations\n",
    "        self.dim = dim\n",
    "\n",
    "        self.slots_mu = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.slots_sigma = nn.Parameter(torch.rand(1, 1, dim))\n",
    "\n",
    "        self.to_keys = torch.nn.Linear(dim, dim)         # from inputs\n",
    "        self.to_queries = torch.nn.Linear(dim, dim)      # from slots\n",
    "        self.to_values = torch.nn.Linear(dim, dim)       # from inputs\n",
    "\n",
    "        self.gru = nn.GRUCell(dim, dim)\n",
    "\n",
    "        hidden_dim = max(dim, hidden_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "        self.layer_norm_inputs = nn.LayerNorm(dim)\n",
    "        self.layer_norm_slots = nn.LayerNorm(dim)\n",
    "        self.layer_norm_pre_ff = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, embeddings: torch.Tensor):\n",
    "        \"\"\"Slot Attention \"\"\"\n",
    "        B, N, D = embeddings.shape\n",
    "        # 1) initialise the slots randomly\n",
    "        mu = self.slots_mu.expand(B, self.num_slots, -1)\n",
    "        sigma = self.slots_sigma.expand(B, self.num_slots, -1)\n",
    "        slots = torch.normal(mu, sigma)\n",
    "\n",
    "        embeddings = self.layer_norm_inputs(embeddings)\n",
    "\n",
    "        keys, values = self.to_keys(embeddings), self.to_values(embeddings)\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            slots_prev = slots\n",
    "            slots = self.layer_norm_slots(slots)\n",
    "\n",
    "            # 2) generate q, k and v vectors using linear projection\n",
    "            #    keys and values are generated from the inputs and \n",
    "            #    queries are generated from the slots\n",
    "            queries = self.to_queries(slots)\n",
    "\n",
    "            # 3) calculate the attention weights between the slots and values\n",
    "            dots = torch.einsum('bid,bjd->bij', queries, keys)\n",
    "            attn = dots.softmax(-1) + 1e-8\n",
    "            attn = attn / attn.sum(dim=-1, keepdim=True)        # scale attention\n",
    "            \n",
    "            # 4) calculate updated slot values by taking a weighted sum of the values\n",
    "            slot_updates = torch.einsum('bjd,bij->bid', values, attn)\n",
    "\n",
    "            # 5) GRU to update slots\n",
    "            slots = self.gru(slot_updates.reshape(-1, D), slots_prev.reshape(-1, D))\n",
    "\n",
    "            slots = slots.reshape(B, -1, D)\n",
    "            slots = slots + self.fc2(F.relu(self.fc1(self.layer_norm_pre_ff(slots))))\n",
    "\n",
    "        return slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabalisticSlotAttention(torch.nn.Module):\n",
    "    def __init__(self, num_slots: int, dim: int, num_iterations: int, hidden_dim: int = 256):\n",
    "        super(SlotAttention, self).__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.num_iterations = num_iterations\n",
    "        self.dim = dim\n",
    "\n",
    "        self.slots_mu = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.slots_sigma = nn.Parameter(torch.rand(1, 1, dim))\n",
    "\n",
    "        self.to_keys = torch.nn.Linear(dim, dim)         # from inputs\n",
    "        self.to_queries = torch.nn.Linear(dim, dim)      # from slots\n",
    "        self.to_values = torch.nn.Linear(dim, dim)       # from inputs\n",
    "\n",
    "        self.gru = nn.GRUCell(dim, dim)\n",
    "\n",
    "        hidden_dim = max(dim, hidden_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
    "\n",
    "        self.layer_norm_inputs = nn.LayerNorm(dim)\n",
    "        self.layer_norm_slots = nn.LayerNorm(dim)\n",
    "        self.layer_norm_pre_ff = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, embeddings: torch.Tensor):\n",
    "        \"\"\"Slot Attention \"\"\"\n",
    "        B, N, D = embeddings.shape\n",
    "        # 1) initialise the slots randomly\n",
    "        mu = self.slots_mu.expand(B, self.num_slots, -1)\n",
    "        sigma = self.slots_sigma.expand(B, self.num_slots, -1)\n",
    "        slots = torch.normal(mu, sigma)\n",
    "\n",
    "        embeddings = self.layer_norm_inputs(embeddings)\n",
    "\n",
    "        keys, values = self.to_keys(embeddings), self.to_values(embeddings)\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            slots_prev = slots\n",
    "            slots = self.layer_norm_slots(slots)\n",
    "\n",
    "            # 2) generate q, k and v vectors using linear projection\n",
    "            #    keys and values are generated from the inputs and \n",
    "            #    queries are generated from the slots\n",
    "            queries = self.to_queries(slots)\n",
    "\n",
    "            # 3) calculate the attention weights between the slots and values\n",
    "            dots = torch.einsum('bid,bjd->bij', queries, keys)\n",
    "            attn = dots.softmax(-1) + 1e-8\n",
    "            attn = attn / attn.sum(dim=-1, keepdim=True)        # scale attention\n",
    "            \n",
    "            # 4) calculate updated slot values by taking a weighted sum of the values\n",
    "            slot_updates = torch.einsum('bjd,bij->bid', values, attn)\n",
    "\n",
    "            # 5) GRU to update slots\n",
    "            slots = self.gru(slot_updates.reshape(-1, D), slots_prev.reshape(-1, D))\n",
    "\n",
    "            slots = slots.reshape(B, -1, D)\n",
    "            slots = slots + self.fc2(F.relu(self.fc1(self.layer_norm_pre_ff(slots))))\n",
    "\n",
    "        return slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "sample_embeddings = torch.randn((2, 10, 512))\n",
    "slot_attention = SlotAttention(3, 512, 3)\n",
    "slots = slot_attention(sample_embeddings)\n",
    "print(slots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertainty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
